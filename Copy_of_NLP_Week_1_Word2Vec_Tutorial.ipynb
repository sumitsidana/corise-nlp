{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziQY_aIHBcLR"
      },
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "\n",
        "### Word2Vec\n",
        "\n",
        "In this notebook we're going to learn and play around with word2vec embeddings that are packaged with Spacy. We'll try to build intuition on how they work and what can we do with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2T-deBX3huM"
      },
      "source": [
        "Install all the required dependencies for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj-w6eV7_abU"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy==2.2.4 --quiet\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt install libopenblas-base libomp-dev\n",
        "!pip install faiss-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnFmq6hM3n4D"
      },
      "source": [
        "Import all the necessary libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSDbwEoL8-0r"
      },
      "source": [
        "from collections import defaultdict\n",
        "import en_core_web_md\n",
        "import numpy as np\n",
        "import spacy\n",
        "import time\n",
        "import faiss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQa8P1KPt4xP"
      },
      "source": [
        "Now let's load the Spacy data which comes with pre-trainined embeddings. This process is expensive so only do this once.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA_Xwhmnt2Ph"
      },
      "source": [
        "spacyModel = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vnj8WIrujBZ"
      },
      "source": [
        "First, let's play with some basic similarity functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnRogtIDuekU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4a15ec-2094-4721-ba8b-574c938a4f97"
      },
      "source": [
        "banana = spacyModel(\"banana\")\n",
        "fruit = spacyModel(\"fruit\")\n",
        "table = spacyModel(\"table\")\n",
        "print(banana.similarity(fruit))\n",
        "print(banana.similarity(table))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.671483588786149\n",
            "0.22562773991991913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp361KZ32XUR"
      },
      "source": [
        "As expected `Banana` is a lot more similar to `Fruit` than to `Table`. Now let's iterate over the entire vocabulary and build a search index using **Faiss**. This will make it a lot faster for us to find similar words instead of having to loop over the entire corpus each time. \n",
        "\n",
        "Feel free to ignore learning more about **Faiss** at this time as we'll dive more into it in Week 3. At the high-level it is a really efficient library to find similar vectors from a corpus.\n",
        "\n",
        "Note: This next cell will take a fair bit of time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNKVP0eYBbZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961dd1c9-691f-48ad-8398-60160211010d"
      },
      "source": [
        "def load_index():\n",
        "  \"\"\"Expensive method - call only once!!\n",
        "  \"\"\"\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  vectors = []\n",
        "  vector_dimension = 300\n",
        "  id = 0\n",
        "\n",
        "  # Iterate over the entire vocabulary\n",
        "  for i, tok in enumerate(spacyModel.vocab):\n",
        "    vector = tok.vector\n",
        "    l2_norm = np.linalg.norm(vector)\n",
        "\n",
        "    # Ignore zero vectors, nan vlaues\n",
        "    if (np.isclose(l2_norm, 0.0) or \n",
        "        np.isnan(l2_norm) or \n",
        "        np.any(np.isnan(vector))):\n",
        "      continue\n",
        "    else:\n",
        "      vectors.append(np.divide(vector, l2_norm))\n",
        "\n",
        "    # Add to the output variables\n",
        "    word_to_id[tok.text.lower()] = id\n",
        "    id_to_word[id] = tok.text.lower()\n",
        "    id += 1\n",
        "\n",
        "  \n",
        "  vectors = np.array(vectors)\n",
        "  index = faiss.IndexFlatIP(vector_dimension)\n",
        "  index.add(vectors)\n",
        "  return word_to_id, id_to_word, vectors, index\n",
        "\n",
        "word_to_id, id_to_word, vectors, index = load_index()\n",
        "vector_size = len(vectors)\n",
        "print(\"We created a search index of %d vectors\" % vector_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We created a search index of 684754 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAQitr3R4bLA"
      },
      "source": [
        "Now we're going to add a helper functions to calculate top_k similar words to some input in the index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8rGyqh1F5O6"
      },
      "source": [
        "def search_vector(word_vector, top_k=100, print_time_taken=False):\n",
        "  word_vector = np.array([word_vector])\n",
        "  start_time = time.time()\n",
        "  scores, indices = index.search(word_vector, top_k)\n",
        "  if print_time_taken:\n",
        "    print(\"Time taken to search amongst {} words is {:.3}s\".format(\n",
        "        vector_size, (time.time() - start_time))\n",
        "    )\n",
        "  results = []\n",
        "  words = set()\n",
        "  for i, query_index in enumerate(indices):\n",
        "      # Matches for the i'th one \n",
        "      for inn_idx, word_index in enumerate(query_index):\n",
        "          if word_index < 0:\n",
        "              continue\n",
        "          word = id_to_word[word_index]\n",
        "          if word in words:\n",
        "            continue\n",
        "          words.add(word)\n",
        "          results.append((word, float(scores[i][inn_idx])))\n",
        "  return sorted(results, key=lambda tup: -tup[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcmoOvfA54JS"
      },
      "source": [
        "Let's do an empirical test by searching similar words to a few terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSriAnCr9g7c"
      },
      "source": [
        "### Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaaWpQ97oCre"
      },
      "source": [
        "def search(word, top_k=100,print_time_taken=False):\n",
        "  word = word.lower()\n",
        "  if word not in word_to_id:\n",
        "    print(\"Oops, the word {} is not in loaded dictionary\".format(word))\n",
        "    return\n",
        "  id = word_to_id[word]\n",
        "  word_vector = vectors[id]\n",
        "  search_results = search_vector(word_vector, top_k, print_time_taken)\n",
        "  print(f\"The top similar words to {word} are - \")\n",
        "  for i in range(len(search_results)):\n",
        "    print(f\"Word = {search_results[i][0]} and similarity = {search_results[i][1]}\")\n",
        "  return search_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iSiVNv7A6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9700c6-7b1c-4976-8c16-f688e3e1868c"
      },
      "source": [
        "output = search(\"happy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to happy are - \n",
            "Word = happy and similarity = 1.0\n",
            "Word = glad and similarity = 0.7701864838600159\n",
            "Word = hope and similarity = 0.7318376898765564\n",
            "Word = everyone and similarity = 0.7277779579162598\n",
            "Word = thankful and similarity = 0.6912474632263184\n",
            "Word = excited and similarity = 0.6901209354400635\n",
            "Word = love and similarity = 0.6869319081306458\n",
            "Word = wish and similarity = 0.6853764653205872\n",
            "Word = appreciative and similarity = 0.6847968101501465\n",
            "Word = greatful and similarity = 0.6847968101501465\n",
            "Word = grateful and similarity = 0.6847968101501465\n",
            "Word = gratefully and similarity = 0.6847968101501465\n",
            "Word = always and similarity = 0.679514467716217\n",
            "Word = lucky and similarity = 0.6788510084152222\n",
            "Word = feel and similarity = 0.6768772006034851\n",
            "Word = friends and similarity = 0.6759893894195557\n",
            "Word = freinds and similarity = 0.6759893894195557\n",
            "Word = thank and similarity = 0.6728549599647522\n",
            "Word = really and similarity = 0.6657895445823669\n",
            "Word = sure and similarity = 0.6657198071479797\n",
            "Word = too and similarity = 0.6657150983810425\n",
            "Word = so and similarity = 0.665570080280304\n",
            "Word = adored and similarity = 0.6629003882408142\n",
            "Word = loved and similarity = 0.6629003882408142\n",
            "Word = loving and similarity = 0.6582800149917603\n",
            "Word = wowed and similarity = 0.6547300219535828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni0bFOxi7CFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7378545-7176-4821-c1cf-540a751822f7"
      },
      "source": [
        "output = search(\"baseball\", 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to baseball are - \n",
            "Word = fastpitch and similarity = 1.0\n",
            "Word = softball and similarity = 1.0\n",
            "Word = baseballs and similarity = 1.0\n",
            "Word = scorebook and similarity = 1.0\n",
            "Word = sandlot and similarity = 1.0\n",
            "Word = baseball and similarity = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IzaN0X67M70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a429b3de-f581-434f-bcc6-72da4d070b56"
      },
      "source": [
        "output = search(\"cheese\", 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to cheese are - \n",
            "Word = cheese and similarity = 0.9999999403953552\n",
            "Word = mozzarella and similarity = 0.9999999403953552\n",
            "Word = fromage and similarity = 0.9999999403953552\n",
            "Word = cheeses and similarity = 0.9999999403953552\n",
            "Word = chevre and similarity = 0.8228567242622375\n",
            "Word = velveeta and similarity = 0.8228567242622375\n",
            "Word = crouton and similarity = 0.8228567242622375\n",
            "Word = chedder and similarity = 0.8228567242622375\n",
            "Word = emmental and similarity = 0.8228567242622375\n",
            "Word = velveta and similarity = 0.8228567242622375\n",
            "Word = cheddar and similarity = 0.8228567242622375\n",
            "Word = chÃ¨vre and similarity = 0.8228567242622375\n",
            "Word = cheeseboard and similarity = 0.8228567242622375\n",
            "Word = part-skim and similarity = 0.8228567242622375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovIxI9Uutgc-"
      },
      "source": [
        "Now why don't you try out a few different words that come to mind and see where does the model perform well and where it struggles!! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVPJ2_R59i-2"
      },
      "source": [
        "### Analogies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bunCp7p9rplw"
      },
      "source": [
        "def analogy(word1, word2, word3):\n",
        "  word1 = word1.lower()\n",
        "  word2 = word2.lower()\n",
        "  word3 = word3.lower()\n",
        "  if word1 not in word_to_id or word2 not in word_to_id or word3 not in word_to_id:\n",
        "    print(\"word not present in dictionary, try something else\")\n",
        "  vector1 = vectors[word_to_id[word1]]\n",
        "  vector2 = vectors[word_to_id[word2]]\n",
        "  vector3 = vectors[word_to_id[word3]]\n",
        "  analogy_results = search_vector(np.add(np.subtract(vector1, vector2), vector3), 10)\n",
        "  print(f\"The top similar item for ({word1} - {word2} + {word3}) = {analogy_results[0][0]}\")\n",
        "  print(f\"The top similar words to ({word1} - {word2} + {word3}) are - \")\n",
        "  for i in range(len(analogy_results)):\n",
        "    print(f\"Word = {analogy_results[i][0]} and similarity = {analogy_results[i][1]}\")\n",
        "  return analogy_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N6uPUoQ5i15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aeb672b-7d4b-4988-dc24-a31c4cba8386"
      },
      "source": [
        "output = analogy(\"king\", \"man\", \"woman\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (king - man + woman) = queen\n",
            "The top similar words to (king - man + woman) are - \n",
            "Word = queen and similarity = 0.8607760071754456\n",
            "Word = king and similarity = 0.8567199110984802\n",
            "Word = highness and similarity = 0.679933488368988\n",
            "Word = prince and similarity = 0.679933488368988\n",
            "Word = commoner and similarity = 0.679933488368988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XkbyKrc7ySI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa6725b-51b7-4349-ac05-c0cb2cbfd1ad"
      },
      "source": [
        "output = analogy(\"smallest\", \"small\", \"short\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (smallest - small + short) = shortest\n",
            "The top similar words to (smallest - small + short) are - \n",
            "Word = shortest and similarity = 0.8045327663421631\n",
            "Word = straightest and similarity = 0.8045327663421631\n",
            "Word = second-longest and similarity = 0.7076637148857117\n",
            "Word = longest-ever and similarity = 0.7076637148857117\n",
            "Word = 200-mile and similarity = 0.7076637148857117\n",
            "Word = longest-running and similarity = 0.7076637148857117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4n1n870tqVS"
      },
      "source": [
        "Now why don't you try out a few different examples see what comes out :) "
      ]
    }
  ]
}